# The Fundamentals of Machine Learning

the main regions and the most notable landmarks of ML:
* supervised or  unsupervised
* online or batch learning
* instance-based or model-based  

definition of ML:

Machine Learning is the science(and art) of programming computers so they can learn from data.  

Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.  --Arthur Samuel,1959  

A comuter program is said to learn from expericence E with respect to some task T and some performance measure P,if its performance on T,as measured by P,improves with experience E.  --Tom Mitchell,1997

ç¬¬ä¸‰ç§å®šä¹‰ç¿»ä¸€ä¸‹ï¼š
ä¸€ä¸ªè®¡ç®—æœºç¨‹åºåˆ©ç”¨ç»éªŒEæ¥å­¦ä¹ ä»»åŠ¡Tï¼Œæ€§èƒ½æ˜¯Pï¼Œå¦‚æœé’ˆå¯¹ä»»åŠ¡Tçš„æ€§èƒ½Péšç€ç»éªŒEä¸æ–­å¢é•¿ï¼Œåˆ™ç§°ä¸ºæœºå™¨å­¦ä¹ ã€‚  

ä¸ºå•¥è¦ç”¨æœºå™¨å­¦ä¹ ï¼Ÿ
å…ˆçœ‹çœ‹ï¼Œä¸ç”¨æœºå™¨å­¦ä¹ çš„è¯ï¼Œæ€ä¹ˆå†™è¯†åˆ«åƒåœ¾é‚®ä»¶çš„ç¨‹åºï¼Ÿ
1. æ‰¾åƒåœ¾é‚®ä»¶çš„ç‰¹ç‚¹
2. ä¸ºåƒåœ¾é‚®ä»¶çš„æ¯ä¸€ä¸ªç‰¹ç‚¹å†™æ£€æµ‹ç®—æ³•
3. æµ‹è¯•ç¨‹åºå¹¶é‡å¤åšä¸Šè¿°ä¸¤éƒ¨ï¼ŒçŸ¥é“ç»“æœè¶³å¤Ÿå¥½ã€‚  

![image-20220303091319550](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303091319550.png)

è¿™ç§åšæ³•æœ‰ä¸ªå¼Šç«¯ï¼š
å¦‚æœé—®é¢˜éå¸¸å¤æ‚ï¼Œä½ éœ€è¦å†™çš„ç”¨æ¥æ£€æµ‹åƒåœ¾é‚®ä»¶çš„è§„åˆ™ä¼šéå¸¸å¤šï¼Œè¿™å¾ˆéš¾ç»´æŠ¤ã€‚  

ç›¸åï¼Œç”¨æœºå™¨å­¦ä¹ æ¥åšåƒåœ¾é‚®ä»¶åˆ†ç±»ï¼Œé€šè¿‡æ¯”è¾ƒæ™®é€šé‚®ä»¶ï¼ˆhamï¼‰å’Œåƒåœ¾é‚®ä»¶ï¼Œè‡ªåŠ¨å­¦ä¹ å‡ºå“ªäº›ç‰¹ç‚¹æ ‡å¿—ç€åƒåœ¾é‚®ä»¶ï¼Œå†™å‡ºçš„ç¨‹åºä¼šæ›´çŸ­ï¼Œæ›´æ˜“äºç»´æŠ¤ï¼Œæ›´å‡†ç¡®ã€‚

![image-20220303091837494](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303091837494.png)

å¦‚æœåƒåœ¾é‚®ä»¶å‘é€è€…ï¼Œé’ˆå¯¹ä½ çš„ä¼ ç»Ÿç®—æ³•è¿›è¡Œè§„é¿ï¼Œæ¯”å¦‚æŠŠä½ èƒ½å¤Ÿæ£€æµ‹åˆ°çš„å…³é”®è¯â€œ4uâ€æ”¹ä¸ºâ€œfor uâ€ï¼Œé‚£ä¹ˆä½ å°±è¦ä¸€ç›´æ›´æ–°ä½ çš„ç®—æ³•ã€‚
è€Œæœºå™¨å­¦ä¹ ç®—æ³•ä¼šè‡ªåŠ¨æ³¨æ„åˆ°â€œfor uâ€åœ¨ç”¨æˆ·æ‰‹åŠ¨æ ‡è®°çš„åƒåœ¾é‚®ä»¶ä¸­é¢‘ç¹å‡ºç°ï¼Œæ— é¡»äººå·¥å¹²é¢„å³å¯è‡ªåŠ¨æ ‡è®°å‡ºåŒ…å«â€œfor uâ€çš„åƒåœ¾é‚®ä»¶ã€‚  

æœºå™¨å­¦ä¹ å¦ä¸€ä¸ªäº®ç‚¹æ˜¯æ“…é•¿å¤„ç†å¯¹äºä¼ ç»Ÿæ–¹æ³•è€Œè¨€å¤ªå¤æ‚æˆ–æ²¡æœ‰å·²çŸ¥ç®—æ³•çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå¯¹äºè¯­éŸ³è¯†åˆ«ï¼Œå‡è®¾ä½ æƒ³å†™ä¸€ä¸ªå¯ä»¥è¯†åˆ« â€œoneâ€å’Œâ€œtwoâ€çš„ç®€å•ç¨‹åºã€‚ä½ å¯èƒ½æ³¨æ„åˆ°â€œtwoâ€çš„èµ·å§‹æ˜¯ä¸€ä¸ªé«˜ éŸ³ï¼ˆâ€œTâ€ï¼‰ï¼Œå› æ­¤ä¼šå†™ä¸€ä¸ªå¯ä»¥æµ‹é‡é«˜éŸ³å¼ºåº¦çš„ç¡¬ç¼–ç ç®—æ³•ï¼Œç”¨äºåŒº åˆ†â€œoneâ€å’Œâ€œtwoâ€ã€‚ä½†æ˜¯å¾ˆæ˜æ˜¾ï¼Œè¿™ä¸ªæ–¹æ³•ä¸èƒ½æ¨å¹¿åˆ°æ‰€æœ‰çš„è¯­éŸ³è¯† åˆ«ï¼ˆäººä»¬æ‰€å¤„ç¯å¢ƒä¸åŒã€è¯­è¨€ä¸åŒã€ä½¿ç”¨çš„è¯æ±‡ä¸åŒï¼‰ã€‚ï¼ˆç°åœ¨ï¼‰æœ€ä½³ çš„æ–¹æ³•æ˜¯æ ¹æ®ç»™å®šçš„å¤§é‡å•è¯å½•éŸ³ï¼Œå†™ä¸€ä¸ªå¯ä»¥è‡ªæˆ‘å­¦ä¹ çš„ç®—æ³•ã€‚

ä¾‹å¦‚ï¼Œå¯¹äºè¯­éŸ³è¯†åˆ«ï¼Œå‡è®¾ä½ æƒ³å†™ä¸€ä¸ªå¯ä»¥è¯†åˆ« â€œoneâ€å’Œâ€œtwoâ€çš„ç®€å•ç¨‹åºã€‚ä½ å¯èƒ½æ³¨æ„åˆ°â€œtwoâ€çš„èµ·å§‹æ˜¯ä¸€ä¸ªé«˜ éŸ³ï¼ˆâ€œTâ€ï¼‰ï¼Œå› æ­¤ä¼šå†™ä¸€ä¸ªå¯ä»¥æµ‹é‡é«˜éŸ³å¼ºåº¦çš„ç¡¬ç¼–ç ç®—æ³•ï¼Œç”¨äºåŒº åˆ†â€œoneâ€å’Œâ€œtwoâ€ã€‚ä½†æ˜¯å¾ˆæ˜æ˜¾ï¼Œè¿™ä¸ªæ–¹æ³•ä¸èƒ½æ¨å¹¿åˆ°æ‰€æœ‰çš„è¯­éŸ³è¯† åˆ«ï¼ˆäººä»¬æ‰€å¤„ç¯å¢ƒä¸åŒã€è¯­è¨€ä¸åŒã€ä½¿ç”¨çš„è¯æ±‡ä¸åŒï¼‰ã€‚ï¼ˆç°åœ¨ï¼‰æœ€ä½³ çš„æ–¹æ³•æ˜¯æ ¹æ®ç»™å®šçš„å¤§é‡å•è¯å½•éŸ³ï¼Œå†™ä¸€ä¸ªå¯ä»¥è‡ªæˆ‘å­¦ä¹ çš„ç®—æ³•ã€‚

æœ€åï¼Œæœºå™¨å­¦ä¹ å¯ä»¥å¸®åŠ©äººç±»è¿›è¡Œå­¦ä¹ ï¼ˆè§å›¾1-4ï¼‰ã€‚æœºå™¨å­¦ä¹ ç®— æ³•å¯ä»¥æ£€æµ‹è‡ªå·±å­¦åˆ°äº†ä»€ä¹ˆï¼ˆå°½ç®¡è¿™å¯¹äºæŸäº›ç®—æ³•å¾ˆæ£˜æ‰‹ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ åƒåœ¾é‚®ä»¶è¿‡æ»¤å™¨è®­ç»ƒäº†è¶³å¤Ÿå¤šçš„åƒåœ¾é‚®ä»¶åï¼Œå°±å¯ä»¥ç”¨å®ƒåˆ—å‡ºåƒåœ¾é‚®ä»¶ é¢„æµ‹å™¨çš„å•è¯å’Œå•è¯ç»„åˆã€‚æœ‰æ—¶å¯èƒ½ä¼šå‘ç°ä¸å¼•äººå…³æ³¨çš„å…³è”æˆ–æ–°è¶‹ åŠ¿ï¼Œè¿™æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£é—®é¢˜ã€‚ä½¿ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æŒ–æ˜å¤§é‡æ•°æ®æ¥å¸®åŠ© å‘ç°ä¸å¤ªæ˜æ˜¾çš„è§„å¾‹ã€‚è¿™ç§°ä½œæ•°æ®æŒ–æ˜ã€‚

![image-20220303092913238](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303092913238.png)

æ€»ç»“ä¸€ä¸‹ï¼š
æœºå™¨å­¦ä¹ é€‚ç”¨äºï¼š
Â·æœ‰è§£å†³æ–¹æ¡ˆï¼ˆä½†è§£å†³æ–¹æ¡ˆéœ€è¦è¿›è¡Œå¤§é‡äººå·¥å¾®è°ƒæˆ–éœ€è¦éµå¾ªå¤§é‡ è§„åˆ™ï¼‰çš„é—®é¢˜ï¼šæœºå™¨å­¦ä¹ ç®—æ³•é€šå¸¸å¯ä»¥ç®€åŒ–ä»£ç ï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰æ›´å¥½ çš„æ€§èƒ½ã€‚
Â·ä¼ ç»Ÿæ–¹æ³•éš¾ä»¥è§£å†³çš„å¤æ‚é—®é¢˜ï¼šæœ€å¥½çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ä¹Ÿè®¸å¯ä»¥æ‰¾ åˆ°è§£å†³æ–¹æ¡ˆã€‚
Â·ç¯å¢ƒæœ‰æ³¢åŠ¨ï¼šæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥é€‚åº”æ–°æ•°æ®ã€‚
Â·æ´å¯Ÿå¤æ‚é—®é¢˜å’Œå¤§é‡æ•°æ®ã€‚


## æœºå™¨å­¦ä¹ èƒ½å¤„ç†çš„é—®é¢˜ï¼š
ä¸€å¤§å †ï¼Œä¸ä¸€ä¸€åˆ—äº†ï¼Œå’Œé¢„æµ‹æœ‰å…³çš„ï¼š
Forecasting your companyâ€™s revenue next year, based on many performance metrics. This a regression task (i.e., predicting values), which may be tackled using any regression model, such as a Linear Regression or Polynomial Regression model (see Chapter 4), a regression SVM (see Chapter 5), a regression random forest (see Chapter 7) or an artificial neural network (see Chapter 10). If you want to take into account sequences of past performance metrics, you may want to use recurrent neural networks (RNNs), convolutional neural networks (CNNs) or Transformers (see Chapter 15 and Chapter 16).

è¿˜æœ‰è¿™ä¸‹é¢è¿™ä¸ªæˆ‘ä¹Ÿè§‰å¾—æ˜¯å¾ˆæ³›ç”¨å¾ˆé…·çš„ä¸œè¥¿ï¼š
Representing a complex, high-dimensional dataset in a clear and insightful diagram: this is data visualization, often involving dimensionality reduction techniques (see Chapter 8).

æ¥äº†æ¥äº†ï¼Œé˜¿å°”æ³•ç‹—ï¼š
Building an intelligent bot for a game. This is often tackled using Reinforcement Learning (RL, see Chapter 18), which is a branch of Machine Learning that trains agents (such as bots) to pick the actions that will maximize their rewards over time (e.g., a bot may get a reward every time the player loses some life points), within a given environment (e.g., the game). The famous AlphaGo program that beat the world champion at the game of go was built using RL.

## Types of Machine Learning Systems
Â·æ˜¯å¦åœ¨äººç±»ç›‘ç£ä¸‹è®­ç»ƒï¼ˆæœ‰ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹  å’Œå¼ºåŒ–å­¦ä¹ ï¼‰ã€‚

Â·æ˜¯å¦å¯ä»¥åŠ¨æ€åœ°è¿›è¡Œå¢é‡å­¦ä¹ ï¼ˆåœ¨çº¿å­¦ä¹ å’Œæ‰¹é‡å­¦ä¹ ï¼‰ã€‚

Â·æ˜¯ç®€å•åœ°å°†æ–°çš„æ•°æ®ç‚¹å’Œå·²çŸ¥çš„æ•°æ®ç‚¹è¿›è¡ŒåŒ¹é…ï¼Œè¿˜æ˜¯åƒç§‘å­¦å®¶ é‚£æ ·ï¼Œå¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ¨¡å¼æ£€æµ‹ç„¶åå»ºç«‹ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼ˆåŸºäºå®ä¾‹çš„å­¦ ä¹ å’ŒåŸºäºæ¨¡å‹çš„å­¦ä¹ ï¼‰ã€‚

For example, a state-of-the-art spam filter may learn on the fly using a deep neural network model trained using examples of spam and ham; this makes it an online, model-based, supervised learning system.

## Supervised/Unsupervised Learning
æ ¹æ®è®­ç»ƒæœŸé—´æ¥å—çš„ç›‘ç£æ•°é‡å’Œç›‘ç£ç±»å‹ï¼Œå¯ä»¥å°†æœºå™¨å­¦ä¹ ç³»ç»Ÿåˆ† ä¸ºä»¥ä¸‹å››ä¸ªä¸»è¦ç±»åˆ«ï¼šæœ‰ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ã€åŠç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚

### Supervised learning
In supervised learning, the training set you feed to the algorithm includes the desired solutions, called **labels** (Figure 1-5).

attribute å’Œ featureçš„åŒºåˆ«ï¼š
In Machine Learning an attribute is a data type (e.g., â€œMileageâ€), while a feature has several meanings, depending on the context, but generally means an attribute plus its value (e.g., â€œMileage = 15,000â€). Many people use the words attribute and feature interchangeably.

ä¸€äº›ç›‘ç£å­¦ä¹ ç®—æ³•ï¼š
k-Nearest
Neighbors 
Linear Regression 
Logistic Regression 
Support Vector Machines (SVMs) 
Decision Trees and Random Forests 
Neural networks

### Unsupervised learning
In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher.
ä¸€äº›æ— ç›‘ç£å­¦ä¹ æŠ€æœ¯ï¼š
Clustering èšç±»
K-Means k-å‡å€¼
DBSCAN 
Hierarchical Cluster Analysis (HCA) åˆ†å±‚èšç±»åˆ†æ 
Anomaly detection and novelty detection å¼‚å¸¸æ£€æµ‹å’Œæ–°é¢–æ€§æ£€æµ‹
One-class SVM å•ç±»SVM
Isolation Forest å­¤ç«‹æ£®æ—
Visualization and dimensionality reduction å¯è§†åŒ–å’Œé™ç»´ 
Principal Component Analysis (PCA) ä¸»æˆåˆ†åˆ†æ
Kernel PCA æ ¸ä¸»æˆåˆ†åˆ†æ
Locally-Linear Embedding (LLE) å±€éƒ¨çº¿æ€§åµŒå…¥ 
t-distributed Stochastic Neighbor Embedding (t-SNE) t-åˆ†å¸ƒéšæœºè¿‘é‚»åµŒå…¥ 
Association rule learning å…³è”è§„åˆ™å­¦ä¹ 
Apriori 
Eclat

å¯è§†åŒ–ç®—æ³•ä¹Ÿæ˜¯æ— ç›‘ç£å­¦ä¹ ç®—æ³•çš„ä¸€ä¸ªä¸é”™çš„ç¤ºä¾‹ï¼šä½ æä¾›å¤§é‡å¤ æ‚çš„ã€æœªæ ‡è®°çš„æ•°æ®ï¼Œç®—æ³•è½»æ¾ç»˜åˆ¶è¾“å‡º2Dæˆ–3Dçš„æ•°æ®è¡¨ç¤ºï¼ˆè§å›¾19ï¼‰ã€‚è¿™äº›ç®—æ³•ä¼šå°½å…¶æ‰€èƒ½åœ°ä¿ç•™å°½é‡å¤šçš„ç»“æ„ï¼ˆä¾‹å¦‚ï¼Œå°è¯•ä¿æŒè¾“å…¥ çš„å•ç‹¬é›†ç¾¤åœ¨å¯è§†åŒ–ä¸­ä¸ä¼šè¢«é‡å ï¼‰ï¼Œä»¥ä¾¿äºä½ ç†è§£è¿™äº›æ•°æ®æ˜¯æ€ä¹ˆç»„ ç»‡çš„ï¼Œç”šè‡³è¯†åˆ«å‡ºä¸€äº›æœªçŸ¥çš„æ¨¡å¼ã€‚

![image-20220303105345916](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303105345916.png)

ä¸ä¹‹ç›¸å…³çš„ä¸€ä¸ªä»»åŠ¡æ˜¯é™ç»´ï¼Œé™ç»´çš„ç›®çš„æ˜¯åœ¨ä¸ä¸¢å¤±å¤ªå¤šä¿¡æ¯çš„å‰ æä¸‹ç®€åŒ–æ•°æ®ã€‚æ–¹æ³•ä¹‹ä¸€æ˜¯å°†å¤šä¸ªç›¸å…³ç‰¹å¾åˆå¹¶ä¸ºä¸€ä¸ªã€‚ä¾‹å¦‚ï¼Œæ±½è½¦é‡Œ ç¨‹ä¸å…¶ä½¿ç”¨å¹´é™å­˜åœ¨å¾ˆå¤§çš„ç›¸å…³æ€§ï¼Œæ‰€ä»¥é™ç»´ç®—æ³•ä¼šå°†å®ƒä»¬åˆå¹¶æˆä¸€ä¸ª ä»£è¡¨æ±½è½¦ç£¨æŸçš„ç‰¹å¾ã€‚è¿™ä¸ªè¿‡ç¨‹å«ä½œç‰¹å¾æå–ã€‚**feature extraction**

## Semisupervised learning
Since labeling data is usually time-consuming and costly, you will often have plenty of unlabeled instances, and few labeled instances. Some algorithms can deal with data thatâ€™s partially labeled. This is called semisupervised learning (Figure 1-11).

![image-20220303112643750](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303112643750.png)

å¤§å¤šæ•°åŠç›‘ç£å­¦ä¹ ç®—æ³•æ˜¯æ— ç›‘ç£ç®—æ³•å’Œæœ‰ç›‘ç£ç®—æ³•çš„ç»“åˆã€‚ä¾‹å¦‚ï¼Œ æ·±åº¦ä¿¡å¿µç½‘ç»œï¼ˆDBNï¼‰åŸºäºä¸€ç§äº’ç›¸å †å çš„æ— ç›‘ç£ç»„ä»¶ï¼Œè¿™ä¸ªç»„ä»¶å«ä½œ å—é™ç»å°”å…¹æ›¼æœºï¼ˆRBMï¼‰ã€‚å—é™ç»å°”å…¹æ›¼æœºä»¥æ— ç›‘ç£æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œç„¶ åä½¿ç”¨æœ‰ç›‘ç£å­¦ä¹ æŠ€æœ¯å¯¹æ•´ä¸ªç³»ç»Ÿè¿›è¡Œå¾®è°ƒã€‚

## Reinforcement Learning
Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.

![image-20220303141311306](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303141311306.png)


DeepMindçš„ AlphaGoé¡¹ç›®ä¹Ÿæ˜¯ä¸€ä¸ªå¼ºåŒ–å­¦ä¹ çš„å¥½ç¤ºä¾‹ã€‚2017å¹´5æœˆï¼ŒAlphaGoåœ¨å›´æ£‹ æ¯”èµ›ä¸­å‡»è´¥ä¸–ç•Œå† å†›æŸ¯æ´è€Œå£°åé¹Šèµ·ã€‚é€šè¿‡åˆ†ææ•°ç™¾ä¸‡åœºæ¯”èµ›ï¼Œç„¶åè‡ª å·±è·Ÿè‡ªå·±ä¸‹æ£‹ï¼Œå®ƒå­¦åˆ°äº†åˆ¶èƒœç­–ç•¥ã€‚è¦æ³¨æ„ï¼Œåœ¨è·Ÿä¸–ç•Œå† å†›å¯¹å¼ˆçš„æ—¶ å€™ï¼ŒAlphaGoå¤„äºå…³é—­å­¦ä¹ çŠ¶æ€ï¼Œå®ƒåªæ˜¯åº”ç”¨å®ƒæ‰€å­¦åˆ°çš„ç­–ç•¥è€Œå·²ã€‚

## Batch and Online Learning
å¦ä¸€ä¸ªç»™æœºå™¨å­¦ä¹ ç³»ç»Ÿåˆ†ç±»çš„æ ‡å‡†æ˜¯çœ‹ç³»ç»Ÿæ˜¯å¦å¯ä»¥ä»ä¼ å…¥çš„æ•°æ® æµä¸­è¿›è¡Œå¢é‡å­¦ä¹ ã€‚

## Batch learning
In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called offline learning.

If you want a batch learning system to know about new data (such as a new type of spam), you need to train a new version of the system from scratch on the full dataset (not just the new data, but also the old data), then stop the old system and replace it with the new one.

Fortunately, the whole process of training, evaluating, and launching a Machine Learning system can be automated fairly easily (as shown in Figure 1-3), so even a batch learning system can adapt to change. Simply update the data and train a new version of the system from scratch as often as needed.

This solution is simple and often works fine, but training using the full set of data can take many hours, so you would typically train a new system only every 24 hours or even just weekly. If your system needs to adapt to rapidly changing data (e.g., to predict stock prices), then you need a more reactive solution.

Also, training on the full set of data requires a lot of computing resources (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and you automate your system to train from scratch every day, it will end up costing you a lot of money. If the amount of data is huge, it may even be impossible to use a batch learning algorithm.

Finally, if your system needs to be able to learn autonomously and it has limited resources (e.g., a smartphone application or a rover on Mars), then carrying around large amounts of training data and taking up a lot of resources to train for hours every day is a showstopper.

Fortunately, a better option in all these cases is to use algorithms that are capable of learning incrementally.

## Online learning
In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives (see Figure 1-13).

![image-20220303143156585](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303143156585.png)

Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously. It is also a good option if you have limited computing resources: once an online learning system has learned about new data instances, it does not need them anymore, so you can discard them (unless you want to be able to roll back to a previous state and â€œreplayâ€ the data). This can save a huge amount of space.

Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machineâ€™s main memory (this is called out-of-core learning). The algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14).

Out-of-core learning is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning.

![image-20220303144007475](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303144007475.png)

One important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data (you donâ€™t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it will learn more slowly, but it will also be less sensitive to noise in the new data or to sequences of nonrepresentative data points (outliers).

A big challenge with online learning is that if bad data is fed to the system, the systemâ€™s performance will gradually decline. If we are talking about a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from someone spamming a search engine to try to rank high in search results. To reduce this risk, you need to monitor your system closely and promptly switch learning off (and possibly revert to a previously working state) if you detect a drop in performance. You may also want to monitor the input data and react to abnormal data (e.g., using an anomaly detection algorithm).

## Instance-Based Versus Model-Based Learning
One more way to categorize Machine Learning systems is by how they generalizeï¼ˆæ³›åŒ–ï¼‰  

Most Machine Learning tasks are about making predictions. This means that given a number of training examples, the system needs to be able to generalize to examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal is to perform well on new instances.

## Instance-based learning
instance-based learning: the system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned examples (or a subset of them). For example, in Figure 1-15 the new instance would be classified as a triangle because the majority of the most similar instances belong to that class.

![image-20220303150437233](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220303150437233.png)

## Model-based learning
Another way to generalize from a set of examples is to build a model of these examples and then use that model to make predictions. This is called model-based learning (Figure 1-16).

æ¥ä¸ªæ —å­
ä½¿ç”¨Scikit-Learnè®­ç»ƒå¹¶è¿è¡Œä¸€ä¸ªçº¿æ€§æ¨¡å‹
```python
import matplotlib.pyplot as plt 
import numpy as np 
import pandas as pd 
import sklearn.linear_model

# Load the data 
oecd_bli = pd.read_csv("oecd_bli_2015.csv", thousands=',') 
gdp_per_capita = pd.read_csv("gdp_per_capita.csv",thousands=',',delimiter='\t',encoding='latin1', na_values="n/a")

# Prepare the data 
country_stats = prepare_country_stats(oecd_bli, gdp_per_capita) X = np.c_[country_stats["GDP per capita"]] y = np.c_[country_stats["Life satisfaction"]]

# Visualize the data

country_stats.plot(kind='scatter', x="GDP per capita", y='Life satisfaction')

plt.show()

# Select a linear model 
model = sklearn.linear_model.LinearRegression()

# Train the model model.fit(X, y)

# Make a prediction for Cyprus 
X_new = [[22587]] # Cyprus' GDP per capita 
print(model.predict(X_new)) # outputs [[ 5.96242338]]
```

## Main Challenges of Machine Learning  
## Insufficient Quentity of Training Data
å¯¹äºå¤æ‚é—®é¢˜è€Œè¨€ï¼Œæ•°æ®æ¯”ç®—æ³•é‡è¦ã€‚

ä½†ï¼šä¸­å°å‹æ•°æ®é›†ä¾ç„¶éå¸¸æ™®éï¼Œè·å¾— é¢å¤–çš„è®­ç»ƒæ•°æ®å¹¶ä¸æ€»æ˜¯ä¸€ä»¶è½»è€Œæ˜“ä¸¾æˆ–ç‰©ç¾ä»·å»‰çš„äº‹æƒ…ï¼Œæ‰€ä»¥æš‚æ—¶å…ˆ ä¸è¦æŠ›å¼ƒç®—æ³•ã€‚

## Nonrepresentative Training Data
ä¸ºäº†å¾ˆå¥½åœ°å®ç°æ³›åŒ–ï¼Œè‡³å…³é‡è¦çš„ä¸€ç‚¹æ˜¯å¯¹äºå°†è¦æ³›åŒ–çš„æ–°ç¤ºä¾‹æ¥ è¯´ï¼Œè®­ç»ƒæ•°æ®ä¸€å®šè¦éå¸¸æœ‰ä»£è¡¨æ€§ã€‚æ— è®ºä½ ä½¿ç”¨çš„æ˜¯åŸºäºå®ä¾‹çš„å­¦ä¹ è¿˜ æ˜¯åŸºäºæ¨¡å‹çš„å­¦ä¹ ï¼Œéƒ½æ˜¯å¦‚æ­¤ã€‚

It is crucial to use a training set that is representative of the cases you want to generalize to. This is often harder than it sounds: if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.ï¼ˆé‡‡æ ·åå·®ï¼‰ã€‚

## Poor-quality Data
## Irrelevant Feautures
## Overfitting the Training Data
Overfitting happens when the model is too complex relative to the amount and noisiness of the training data

Here are possible solutions:
Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model.

Gather more training data.

Reduce the noise in the training data (e.g., fix data errors and remove outliers).

Constraining a model to make it simpler and reduce the risk of overfitting is called **regularization**.æ­£åˆ™åŒ–

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å‰é¢å®šä¹‰çš„çº¿æ€§æ¨¡å‹æœ‰ä¸¤ä¸ªå‚æ•°ï¼šÎ¸0å’ŒÎ¸1ã€‚å› æ­¤ï¼Œ è¯¥ç®—æ³•åœ¨æ‹Ÿåˆè®­ç»ƒæ•°æ®æ—¶ï¼Œè°ƒæ•´æ¨¡å‹çš„è‡ªç”±åº¦å°±ç­‰äº2ï¼Œå®ƒå¯ä»¥è°ƒæ•´çº¿ çš„é«˜åº¦ï¼ˆÎ¸0 ï¼‰å’Œæ–œç‡ï¼ˆÎ¸1 ï¼‰ã€‚å¦‚æœæˆ‘ä»¬å¼ºè¡Œè®©Î¸1 =0ï¼Œé‚£ä¹ˆç®—æ³•çš„è‡ª ç”±åº¦å°†ä¼šé™ä¸º1ï¼Œå¹¶ä¸”æ‹Ÿåˆæ•°æ®å°†å˜å¾—æ›´ä¸ºè‰°éš¾â€”â€”å®ƒèƒ½åšçš„å…¨éƒ¨å°±åª æ˜¯å°†çº¿ä¸Šç§»æˆ–ä¸‹ç§»æ¥å°½é‡æ¥è¿‘è®­ç»ƒå®ä¾‹ï¼Œæœ€åææœ‰å¯èƒ½åœç•™åœ¨å¹³å‡å€¼é™„ è¿‘ã€‚è¿™ç¡®å®å¤ªç®€å•äº†ï¼å¦‚æœæˆ‘ä»¬å…è®¸ç®—æ³•ä¿®æ”¹Î¸1 ï¼Œä½†æ˜¯æˆ‘ä»¬å¼ºåˆ¶å®ƒåª èƒ½æ˜¯å¾ˆå°çš„å€¼ï¼Œé‚£ä¹ˆç®—æ³•çš„è‡ªç”±åº¦å°†ä½äº1å’Œ2ä¹‹é—´ï¼Œè¿™ä¸ªæ¨¡å‹å°†ä¼šæ¯”è‡ª ç”±åº¦ä¸º2çš„æ¨¡å‹ç¨å¾®ç®€å•ä¸€äº›ï¼ŒåŒæ—¶åˆæ¯”è‡ªç”±åº¦ä¸º1çš„æ¨¡å‹ç•¥å¾®å¤æ‚ä¸€ äº›ã€‚ä½ éœ€è¦åœ¨å®Œç¾åŒ¹é…æ•°æ®å’Œä¿æŒæ¨¡å‹ç®€å•ä¹‹é—´æ‰¾åˆ°åˆé€‚çš„å¹³è¡¡ç‚¹ï¼Œä» è€Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°æ³›åŒ–ã€‚

ä¸€ä¸ªå…·ä½“æ —å­çœ‹çœ‹æ­£åˆ™åŒ–ä½œç”¨ï¼š
å›¾1-23æ˜¾ç¤ºäº†ä¸‰ä¸ªæ¨¡å‹ã€‚ç‚¹çº¿è¡¨ç¤ºçš„æ˜¯åœ¨ä»¥åœ†åœˆè¡¨ç¤ºçš„å›½å®¶ä¸Šè®­ç»ƒ çš„åŸå§‹æ¨¡å‹ï¼ˆæ²¡æœ‰æ­£æ–¹å½¢è¡¨ç¤ºçš„å›½å®¶ï¼‰ï¼Œè™šçº¿æ˜¯æˆ‘ä»¬åœ¨æ‰€æœ‰å›½å®¶ï¼ˆåœ†åœˆ å’Œæ–¹å½¢ï¼‰ä¸Šè®­ç»ƒçš„ç¬¬äºŒä¸ªæ¨¡å‹ï¼Œå®çº¿æ˜¯ç”¨ä¸ç¬¬ä¸€ä¸ªæ¨¡å‹ç›¸åŒçš„æ•°æ®è®­ç»ƒ çš„æ¨¡å‹ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªæ­£åˆ™åŒ–çº¦æŸã€‚å¯ä»¥çœ‹åˆ°ï¼Œæ­£åˆ™åŒ–å¼ºåˆ¶äº†æ¨¡å‹çš„æ–œç‡ è¾ƒå°ï¼šè¯¥æ¨¡å‹ä¸è®­ç»ƒæ•°æ®ï¼ˆåœ†åœˆï¼‰çš„æ‹Ÿåˆä¸å¦‚ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œä½†å®ƒå®é™…ä¸Š æ›´å¥½åœ°æ³›åŒ–äº†å®ƒæ²¡æœ‰åœ¨è®­ç»ƒæ—¶çœ‹åˆ°çš„æ–°å®ä¾‹ï¼ˆæ–¹å½¢ï¼‰ã€‚

![image-20220304085811028](https://raw.githubusercontent.com/lunnche/picgo-image/main/image-20220304085811028.png)

åœ¨å­¦ä¹ æ—¶ï¼Œåº”ç”¨æ­£åˆ™åŒ–çš„ç¨‹åº¦å¯ä»¥é€šè¿‡ä¸€ä¸ªè¶…å‚æ•°æ¥æ§åˆ¶ã€‚è¶…å‚æ•° æ˜¯å­¦ä¹ ç®—æ³•ï¼ˆä¸æ˜¯æ¨¡å‹ï¼‰çš„å‚æ•°ã€‚å› æ­¤ï¼Œå®ƒä¸å—ç®—æ³•æœ¬èº«çš„å½±å“ã€‚è¶…å‚ æ•°å¿…é¡»åœ¨è®­ç»ƒä¹‹å‰è®¾ç½®å¥½ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜ã€‚å¦‚æœå°†æ­£åˆ™åŒ–è¶… å‚æ•°è®¾ç½®ä¸ºéå¸¸å¤§çš„å€¼ï¼Œä¼šå¾—åˆ°ä¸€ä¸ªå‡ ä¹å¹³å¦çš„æ¨¡å‹ï¼ˆæ–œç‡æ¥è¿‘é›¶ï¼‰ã€‚ å­¦ä¹ ç®—æ³•è™½ç„¶è‚¯å®šä¸ä¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œä½†æ˜¯ä¹Ÿæ›´åŠ ä¸å¯èƒ½æ‰¾åˆ°ä¸€ä¸ªå¥½ çš„è§£å†³æ–¹æ¡ˆã€‚è°ƒæ•´è¶…å‚æ•°æ˜¯æ„å»ºæœºå™¨å­¦ä¹ ç³»ç»Ÿéå¸¸é‡è¦çš„ç»„æˆéƒ¨åˆ†

## Underfitting the Training Data
ï¼Œæ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆæ­£å¥½ç›¸åã€‚å®ƒçš„äº§ç”Ÿé€šå¸¸æ˜¯å›  ä¸ºå¯¹äºåº•å±‚çš„æ•°æ®ç»“æ„æ¥è¯´ï¼Œä½ çš„æ¨¡å‹å¤ªè¿‡ç®€å•ã€‚ä¾‹å¦‚ï¼Œç”¨çº¿æ€§æ¨¡å‹æ¥ æè¿°ç”Ÿæ´»æ»¡æ„åº¦å°±å±äºæ¬ æ‹Ÿåˆã€‚ç°å®æƒ…å†µè¿œæ¯”æ¨¡å‹å¤æ‚å¾—å¤šï¼Œæ‰€ä»¥å³ä¾¿ æ˜¯å¯¹äºç”¨æ¥è®­ç»ƒçš„ç¤ºä¾‹ï¼Œè¯¥æ¨¡å‹äº§ç”Ÿçš„é¢„æµ‹éƒ½ä¸€å®šæ˜¯ä¸å‡†ç¡®çš„ã€‚

è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸»è¦æ–¹å¼æœ‰ï¼š

Â·é€‰æ‹©ä¸€ä¸ªå¸¦æœ‰æ›´å¤šå‚æ•°ã€æ›´å¼ºå¤§çš„æ¨¡å‹ã€‚

Â·ç»™å­¦ä¹ ç®—æ³•æä¾›æ›´å¥½çš„ç‰¹å¾é›†ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰ã€‚

Â·å‡å°‘æ¨¡å‹ä¸­çš„çº¦æŸï¼ˆä¾‹å¦‚ï¼Œå‡å°‘æ­£åˆ™åŒ–è¶…å‚æ•°ï¼‰ã€‚

## Stepping Back
letâ€™s step back and look at the big picture:

Machine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.

There are many different types of ML systems: supervised or not, batch or online, instance-based or modelbased.

In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based, it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by using a similarity measure to compare them to the learned instances.

The system will not perform well if your training set is too small, or if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).

## Testing and Validating

split your data into two sets:training set and test set.

The error on new cases is called the generalization error(or out-of=sample error)

**If the training error is low(i.e. your model makes few mistakes on the training set) but the generalization error is high,it means that your model is overfitting the training data**

It is common to use 80% of the training and hold out 20% for testing.ä½†å¦‚æœæ ·æœ¬æœ‰10millionï¼Œé‚£testing ç•™1%ä¹Ÿæ˜¯è¶³å¤Ÿçš„ã€‚

## Hyperparameter Tuning and Model Selection
è§£å†³æŸä¸ªé—®é¢˜ï¼Œæ€ä¹ˆå†³å®šç”¨å“ªç§æ¨¡å‹å‘¢ï¼Œç”¨linear modelè¿˜æ˜¯polynomial model?éƒ½å»è®­ç»ƒç„¶åæ¯”è°åœ¨test dataä¸Šè¡¨ç°å¥½ã€‚  

æƒ³å¯¹æ¨¡å‹æ­£åˆ™åŒ–æ¥é¿å…è¿‡æ‹Ÿåˆï¼Œæ€ä¹ˆç¡®å®šregularization hyperparameter?
ä¸€ç§æƒ³æ³•æ˜¯è¶…å‚è¯•100ä¸ªæ•°ï¼Œé€‰å‡ºæ³›åŒ–è¯¯å·®æœ€å°çš„é‚£ä¸ªï¼Œæ¯”å¦‚5%çš„è¯¯å·®ï¼Œä½†ä¹‹åä½ çš„æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ä¼šå‘ç°è¯¯å·®å¯èƒ½å˜ä¸º15%äº†ï¼Œwhyï¼Ÿ
The problem is that you measured the generaliztion error multiple times on the test set,ä½ æŠŠç²¾åŠ›éƒ½èŠ±åœ¨å¦‚æœè®©ä½ çš„æ¨¡å‹å’Œå‚æ•°åœ¨è¿™ä¸ªtest setä¸Šè¡¨ç°å¾—å¥½äº†ï¼Œä½ çš„æ¨¡å‹è¿‡äºâ€œä¸“â€äº†ï¼Œæ³›åŒ–å°±åšä¸å¥½äº†ã€‚

è§£å†³è¿™ä¸€é—®é¢˜çš„åŠæ³•å«åšï¼šholdout validation
æŠŠè®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†å˜æˆéªŒè¯é›†ï¼š
hold out part of the training set to evaluate several candidate models and select the best one.The new heldout set is called the validation set(or sometimes the development set,or dev set).

you train multiple models with various hyperparameters on the reduced training set(i.e. the full training set minus the validation set),and you select the model that performs best on the validation set.After this holdout validation process,**you train the best model on the full training set(including the validation set)**,and this gives you the final model.
Lastly,you evaluate this final model on the test set to get an estimate of the generalization error.  

ä¸Šè¿°åšæ³•é€šå¸¸æœ‰æ•ˆï¼Œä½†å¦‚æœéªŒè¯é›†è®¾ç½®å¾—è¿‡å°ï¼Œé‚£å°±ä¸ç®¡ç”¨äº†ï¼Œå¦‚æœéªŒè¯é›†å¤ªå¤§ï¼Œå‰©ä¸‹çš„training setå°±å°äº†ï¼Œä¸ºå•¥è¿™æ ·ä¸å¥½ï¼šsince the final model will be trained on the full training set,it is not ideal to compare candidate models trained on a much smaller training set.

è§£å†³ä¸Šè¿°é—®é¢˜çš„åŠæ³•æ˜¯perform repeated **cross-validation**:è®¾ç½®å¤šä¸ªå°éªŒè¯é›†ï¼Œæ¯ä¸ªå€™é€‰æ¨¡å‹éƒ½åœ¨æ¯ä¸€ä¸ªå°éªŒè¯é›†ä¸ŠéªŒè¯ä¸€æ¬¡ï¼Œæ¯ä¸ªæ¨¡å‹åœ¨æ¯ä¸ªå°éªŒè¯é›†ä¸Šéƒ½éªŒè¯åå–å¹³å‡ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ä¸€ä¸ªå‡†ç¡®çš„ç»“æœï¼Œ  ä½†è¿™æ ·æ—¶é—´æˆæœ¬å°†åŠ å¤§ã€‚  

## Data Mismatch  
æœ‰æ—¶å€™ï¼Œä½ è®­ç»ƒæ•°æ®éå¸¸å¤šï¼Œä½†å’Œç”Ÿäº§ä¸­å®é™…æ•°æ®won't be perfectly representative.è¿™ç§æƒ…å†µä¸‹ä¼˜å…ˆè¦ä¿è¯validation å’Œ testé›†é‡Œçš„æ•°æ®æ˜¯representativeçš„ã€‚ä½ å¯ä»¥è®©å®ƒä»¬ä»…åŒ…å«representative data.
ä½ å¯ä»¥æŠŠrepresentative dataæ´—ä¹±ï¼Œä¸€åŠæ”¾åœ¨éªŒè¯é›†ä¸­ï¼Œå¦ä¸€åŠæ”¾åœ¨æµ‹è¯•é›†ä¸­(ç¡®ä¿æ²¡æœ‰é‡å¤æˆ–æ¥è¿‘é‡å¤çš„ç»“æœå‡ºç°åœ¨ä¸¤ä¸ªé›†ä¸­)ã€‚


æ¥æ¥æ¥ çœ‹çœ‹ä»€ä¹ˆæ˜¯å¥—å¨ƒ  testä¸å¤Ÿç”¨ï¼Œæ­£ä¸Švalidation,validationä¸å¤Ÿç”¨ï¼Œå†æ•´ä¸Štrain-dev set:

ä½ è¦åšä¸ªapp ï¼Œè¾“å…¥æ‹çš„èŠ±çš„ç…§ç‰‡ï¼Œè¾“å‡ºå‘Šè¯‰ä½ æ˜¯å•¥èŠ±ï¼Œä½ ä»ç½‘ä¸Šä¸‹äº›èŠ±èŠ±ğŸŒºçš„ç…§ç‰‡è®­ç»ƒï¼Œè¿™äº›ç½‘èŠ±ç…§ç‰‡å’Œç›¸æœºæ’å‡ºçš„èŠ±å¾ˆå¯èƒ½æ˜¯ä¸ç›¸å…³çš„ï¼Œä½ éœ€è¦ç¡®ä¿validation set å’Œ test seté‡Œæœ‰ä¸”ä»…æœ‰å’Œç›¸æœºèŠ±ç›¸å…³çš„ç½‘èŠ±ç…§ç‰‡ã€‚ç„¶åå‘¢ä½ å¯èƒ½é‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼šif you observe that the performance of your model on the validation set is disappointing,you will not know whether this is because your model has overfit the training set,or whether this is just due to the mismatch between the web pictures and the mobile app pictures.
è§£å†³æ–¹æ³•æ˜¯å•¥ï¼šæŠŠä¸€éƒ¨åˆ†ç½‘èŠ±ç…§ç‰‡è®¾ç½®æˆtrain-dev setã€‚å…ˆåœ¨å…¶ä¸Ševaluateï¼Œæ’é™¤è¿‡æ‹Ÿåˆçš„é—®é¢˜ï¼Œé‚£å°±æ˜¯mismatchçš„é—®é¢˜äº†ã€‚
æ€ä¹ˆè§£å†³ï¼Ÿå¯¹ç½‘èŠ±è¿›è¡Œé¢„å¤„ç†è®©å®ƒä»¬çœ‹èµ·æ¥æ›´åƒç›¸æœºèŠ±ï¼Œ
ç›¸ååœ°ï¼Œå¦‚æœåœ¨train-dev setä¸ŠéªŒè¯ç»“æœå°±ä¸å¥½ï¼Œé‚£å°±æ˜¯overfittingçš„é—®é¢˜ï¼Œæ€ä¹ˆè§£å†³ï¼šç®€åŒ–ï¼Œæ­£åˆ™åŒ–ä½ çš„æ¨¡å‹ï¼Œå°è¯•è·å–æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼Œæ¸…ç†è®­ç»ƒæ•°æ®ã€‚  

## No Free Lunch Theorem
A model is a simplified version of the observations.
The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances.
éœ€è¦ä¿ç•™ä»€ä¹ˆæ•°æ®ï¼Œèˆå¼ƒä»€ä¹ˆæ•°æ®ï¼Œä½ éœ€è¦åšassumptions.  
assumptionå•¥ï¼Œæ¯”å¦‚è¯´ a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise,which can safely be ignored.

åœ¨1996å¹´çš„ä¸€ç¯‡è‘—åè®ºæ–‡ä¸­ï¼ŒDavid Wolpertè¯æ˜äº†å¦‚æœä½ å¯¹æ•°æ®å®Œå…¨ä¸åšä»»ä½•å‡è®¾ï¼Œé‚£ä¹ˆå°±æ²¡æœ‰ç†ç”±é€‰æ‹©ä¸€ç§æ¨¡å‹è€Œä¸æ˜¯å…¶ä»–æ¨¡å‹ã€‚è¿™å°±æ˜¯æ‰€è°“çš„â€œæ²¡æœ‰å…è´¹çš„åˆé¤â€å®šç†ã€‚å¯¹äºæŸäº›æ•°æ®é›†ï¼Œæœ€å¥½çš„æ¨¡å‹æ˜¯çº¿æ€§æ¨¡å‹ï¼Œè€Œå¯¹äºå…¶ä»–æ•°æ®é›†ï¼Œæœ€å¥½çš„æ¨¡å‹æ˜¯ç¥ç»ç½‘ç»œã€‚æ²¡æœ‰ä¸€ä¸ªæ¨¡å‹æ˜¯å…ˆå¤©ä¿è¯æ›´å¥½åœ°å·¥ä½œçš„(è¿™å°±æ˜¯è¿™ä¸ªå®šç†çš„åå­—)ã€‚ç¡®å®šå“ªä¸ªæ¨¡å‹æ˜¯æœ€å¥½çš„å”¯ä¸€æ–¹æ³•æ˜¯å¯¹æ‰€æœ‰æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ç”±äºè¿™æ˜¯ä¸å¯èƒ½çš„ï¼Œæ‰€ä»¥åœ¨å®è·µä¸­,ä½ å¯¹æ•°æ®åšäº†ä¸€äº›åˆç†çš„å‡è®¾ï¼Œåªè¯„ä¼°äº†å‡ ä¸ªåˆç†çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œå¯¹äºç®€å•çš„ä»»åŠ¡ï¼Œæ‚¨å¯ä»¥è¯„ä¼°å…·æœ‰ä¸åŒçº§åˆ«æ­£åˆ™åŒ–çš„çº¿æ€§æ¨¡å‹ï¼Œè€Œå¯¹äºå¤æ‚çš„é—®é¢˜ï¼Œæ‚¨å¯ä»¥è¯„ä¼°ä¸åŒçš„ç¥ç»ç½‘ç»œã€‚


